{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61b03a74",
   "metadata": {},
   "source": [
    "# Lunar Lander"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1286a5c6",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stable-baselines3[extra] in /home/prince/anaconda3/envs/tf-gpu/lib/python3.9/site-packages (1.5.0)\n",
      "Requirement already satisfied: pandas in /home/prince/anaconda3/envs/tf-gpu/lib/python3.9/site-packages (from stable-baselines3[extra]) (1.4.2)\n",
      "Requirement already satisfied: matplotlib in /home/prince/anaconda3/envs/tf-gpu/lib/python3.9/site-packages (from stable-baselines3[extra]) (3.5.2)\n",
      "Requirement already satisfied: cloudpickle in /home/prince/anaconda3/envs/tf-gpu/lib/python3.9/site-packages (from stable-baselines3[extra]) (2.1.0)\n",
      "Requirement already satisfied: torch>=1.8.1 in /home/prince/anaconda3/envs/tf-gpu/lib/python3.9/site-packages (from stable-baselines3[extra]) (1.12.0)\n",
      "Requirement already satisfied: numpy in /home/prince/anaconda3/envs/tf-gpu/lib/python3.9/site-packages (from stable-baselines3[extra]) (1.22.3)\n",
      "Requirement already satisfied: gym==0.21 in /home/prince/anaconda3/envs/tf-gpu/lib/python3.9/site-packages (from stable-baselines3[extra]) (0.21.0)\n",
      "Requirement already satisfied: ale-py~=0.7.4 in /home/prince/anaconda3/envs/tf-gpu/lib/python3.9/site-packages (from stable-baselines3[extra]) (0.7.5)\n",
      "Requirement already satisfied: tensorboard>=2.2.0 in /home/prince/anaconda3/envs/tf-gpu/lib/python3.9/site-packages (from stable-baselines3[extra]) (2.4.0)\n",
      "Requirement already satisfied: autorom[accept-rom-license]~=0.4.2 in /home/prince/anaconda3/envs/tf-gpu/lib/python3.9/site-packages (from stable-baselines3[extra]) (0.4.2)\n",
      "Requirement already satisfied: pillow in /home/prince/anaconda3/envs/tf-gpu/lib/python3.9/site-packages (from stable-baselines3[extra]) (7.2.0)\n",
      "Requirement already satisfied: opencv-python in /home/prince/anaconda3/envs/tf-gpu/lib/python3.9/site-packages (from stable-baselines3[extra]) (4.6.0.66)\n",
      "Requirement already satisfied: psutil in /home/prince/anaconda3/envs/tf-gpu/lib/python3.9/site-packages (from stable-baselines3[extra]) (5.9.1)\n",
      "Requirement already satisfied: importlib-resources in /home/prince/anaconda3/envs/tf-gpu/lib/python3.9/site-packages (from ale-py~=0.7.4->stable-baselines3[extra]) (5.8.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.10.0 in /home/prince/anaconda3/envs/tf-gpu/lib/python3.9/site-packages (from ale-py~=0.7.4->stable-baselines3[extra]) (4.11.3)\n",
      "Requirement already satisfied: click in /home/prince/anaconda3/envs/tf-gpu/lib/python3.9/site-packages (from autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (8.0.4)\n",
      "Requirement already satisfied: requests in /home/prince/anaconda3/envs/tf-gpu/lib/python3.9/site-packages (from autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (2.28.0)\n",
      "Requirement already satisfied: tqdm in /home/prince/anaconda3/envs/tf-gpu/lib/python3.9/site-packages (from autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (4.64.0)\n",
      "Requirement already satisfied: AutoROM.accept-rom-license in /home/prince/anaconda3/envs/tf-gpu/lib/python3.9/site-packages (from autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (0.4.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/prince/anaconda3/envs/tf-gpu/lib/python3.9/site-packages (from importlib-metadata>=4.10.0->ale-py~=0.7.4->stable-baselines3[extra]) (3.8.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/prince/anaconda3/envs/tf-gpu/lib/python3.9/site-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (0.4.4)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /home/prince/anaconda3/envs/tf-gpu/lib/python3.9/site-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (1.35.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/prince/anaconda3/envs/tf-gpu/lib/python3.9/site-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (3.3.4)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /home/prince/anaconda3/envs/tf-gpu/lib/python3.9/site-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (3.20.1)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /home/prince/anaconda3/envs/tf-gpu/lib/python3.9/site-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (1.42.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/prince/anaconda3/envs/tf-gpu/lib/python3.9/site-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (61.2.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/prince/anaconda3/envs/tf-gpu/lib/python3.9/site-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/prince/anaconda3/envs/tf-gpu/lib/python3.9/site-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (1.6.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/prince/anaconda3/envs/tf-gpu/lib/python3.9/site-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (0.15.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /home/prince/anaconda3/envs/tf-gpu/lib/python3.9/site-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (0.37.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/prince/anaconda3/envs/tf-gpu/lib/python3.9/site-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (2.0.3)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/prince/anaconda3/envs/tf-gpu/lib/python3.9/site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->stable-baselines3[extra]) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/prince/anaconda3/envs/tf-gpu/lib/python3.9/site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->stable-baselines3[extra]) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/prince/anaconda3/envs/tf-gpu/lib/python3.9/site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->stable-baselines3[extra]) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/prince/anaconda3/envs/tf-gpu/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->stable-baselines3[extra]) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/prince/anaconda3/envs/tf-gpu/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=2.2.0->stable-baselines3[extra]) (0.4.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/prince/anaconda3/envs/tf-gpu/lib/python3.9/site-packages (from requests->autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (2022.6.15)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/prince/anaconda3/envs/tf-gpu/lib/python3.9/site-packages (from requests->autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/prince/anaconda3/envs/tf-gpu/lib/python3.9/site-packages (from requests->autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/prince/anaconda3/envs/tf-gpu/lib/python3.9/site-packages (from requests->autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (1.26.9)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/prince/anaconda3/envs/tf-gpu/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->stable-baselines3[extra]) (3.2.0)\n",
      "Requirement already satisfied: typing-extensions in /home/prince/anaconda3/envs/tf-gpu/lib/python3.9/site-packages (from torch>=1.8.1->stable-baselines3[extra]) (4.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/prince/anaconda3/envs/tf-gpu/lib/python3.9/site-packages (from matplotlib->stable-baselines3[extra]) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/prince/anaconda3/envs/tf-gpu/lib/python3.9/site-packages (from matplotlib->stable-baselines3[extra]) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/prince/anaconda3/envs/tf-gpu/lib/python3.9/site-packages (from matplotlib->stable-baselines3[extra]) (4.25.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/prince/anaconda3/envs/tf-gpu/lib/python3.9/site-packages (from matplotlib->stable-baselines3[extra]) (3.0.9)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/prince/anaconda3/envs/tf-gpu/lib/python3.9/site-packages (from matplotlib->stable-baselines3[extra]) (1.4.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/prince/anaconda3/envs/tf-gpu/lib/python3.9/site-packages (from matplotlib->stable-baselines3[extra]) (21.3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytz>=2020.1 in /home/prince/anaconda3/envs/tf-gpu/lib/python3.9/site-packages (from pandas->stable-baselines3[extra]) (2022.1)\n",
      "Requirement already satisfied: pyglet in /home/prince/anaconda3/envs/tf-gpu/lib/python3.9/site-packages (1.5.26)\n"
     ]
    }
   ],
   "source": [
    "!pip install stable-baselines3[extra]\n",
    "!pip install pyglet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e90efcd",
   "metadata": {},
   "source": [
    "### Install Swig\n",
    "\n",
    "SWIG Installation: https://sourceforge.net/projects/swig/\n",
    "\n",
    "Ubuntu/Linux: https://www.howtoinstall.me/ubuntu/18-04/swig/\n",
    "\n",
    "MacBook: ``brew install swig``\n",
    "\n",
    "Extra doc: https://github.com/pybox2d/pybox2d/blob/master/INSTALL.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b09a1cad",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym[box2d] in /home/prince/anaconda3/envs/tf-gpu/lib/python3.9/site-packages (0.21.0)\r\n",
      "Requirement already satisfied: numpy>=1.18.0 in /home/prince/anaconda3/envs/tf-gpu/lib/python3.9/site-packages (from gym[box2d]) (1.22.3)\r\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/prince/anaconda3/envs/tf-gpu/lib/python3.9/site-packages (from gym[box2d]) (2.1.0)\r\n",
      "Requirement already satisfied: pyglet>=1.4.0 in /home/prince/anaconda3/envs/tf-gpu/lib/python3.9/site-packages (from gym[box2d]) (1.5.26)\r\n",
      "Requirement already satisfied: box2d-py==2.3.5 in /home/prince/anaconda3/envs/tf-gpu/lib/python3.9/site-packages (from gym[box2d]) (2.3.5)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install gym[box2d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02d669f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pygame in /home/prince/anaconda3/envs/tf-gpu/lib/python3.9/site-packages (2.1.2)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05a44cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: box2d-py in /home/prince/anaconda3/envs/tf-gpu/lib/python3.9/site-packages (2.3.5)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install box2d-py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5ac56a",
   "metadata": {},
   "source": [
    "### 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91da9971",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3 import DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72f992a",
   "metadata": {},
   "source": [
    "### 2. Load Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f405a8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba28fe0",
   "metadata": {},
   "source": [
    "### 3. Test Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0926eb3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 Score: -156.48096243698\n",
      "Episode: 1 Score: -55.87547921207003\n",
      "Episode: 2 Score: -68.0280130859886\n",
      "Episode: 3 Score: -121.66057392152138\n",
      "Episode: 4 Score: -134.8971213924894\n"
     ]
    }
   ],
   "source": [
    "episodes = 5\n",
    "\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()\n",
    "    reward_cnt = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        state = new_state\n",
    "        \n",
    "        reward_cnt += reward\n",
    "        \n",
    "    print(f\"Episode: {episode} Score: {reward_cnt}\")\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d39e291",
   "metadata": {},
   "source": [
    "#### 3.1 Action Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a60ad6d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(4)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e451f231",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.88401794, -0.14644936,  0.56038874, -0.13590546, -1.4344269 ,\n",
       "         2.2873518 ,  1.        ,  0.        ], dtype=float32),\n",
       " -100,\n",
       " True,\n",
       " {})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28030390",
   "metadata": {},
   "source": [
    "#### 3.2 Observation Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5761f734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([-inf -inf -inf -inf -inf -inf -inf -inf], [inf inf inf inf inf inf inf inf], (8,), float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30efa5cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.5916364 , -0.96207124,  0.4644716 ,  0.07968568,  0.37826055,\n",
       "        0.3829114 , -0.9656692 ,  0.7205434 ], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb25f487",
   "metadata": {},
   "source": [
    "### 4. Train RL Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7476c128",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'train/logs'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_path = os.path.join(\"train\", \"logs\")\n",
    "log_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf662764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"LunarLander-v2\")\n",
    "env = DummyVecEnv([lambda: env])\n",
    "model = DQN(policy=\"MlpPolicy\", env=env, tensorboard_log=log_path, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0ae2e05e",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-09 13:02:07.453070: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to train/logs/DQN_1\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.969    |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 221      |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 327      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.929    |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 469      |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 747      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.896    |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 643      |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 1096     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.862    |\n",
      "| time/               |          |\n",
      "|    episodes         | 16       |\n",
      "|    fps              | 795      |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 1453     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.831    |\n",
      "| time/               |          |\n",
      "|    episodes         | 20       |\n",
      "|    fps              | 916      |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 1778     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.799    |\n",
      "| time/               |          |\n",
      "|    episodes         | 24       |\n",
      "|    fps              | 1044     |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 2112     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.763    |\n",
      "| time/               |          |\n",
      "|    episodes         | 28       |\n",
      "|    fps              | 1176     |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 2490     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.736    |\n",
      "| time/               |          |\n",
      "|    episodes         | 32       |\n",
      "|    fps              | 1255     |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 2783     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.698    |\n",
      "| time/               |          |\n",
      "|    episodes         | 36       |\n",
      "|    fps              | 1373     |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 3177     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.668    |\n",
      "| time/               |          |\n",
      "|    episodes         | 40       |\n",
      "|    fps              | 1446     |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 3498     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.632    |\n",
      "| time/               |          |\n",
      "|    episodes         | 44       |\n",
      "|    fps              | 1519     |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 3870     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.595    |\n",
      "| time/               |          |\n",
      "|    episodes         | 48       |\n",
      "|    fps              | 1606     |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 4264     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.558    |\n",
      "| time/               |          |\n",
      "|    episodes         | 52       |\n",
      "|    fps              | 1676     |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 4656     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.525    |\n",
      "| time/               |          |\n",
      "|    episodes         | 56       |\n",
      "|    fps              | 1746     |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 5001     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.494    |\n",
      "| time/               |          |\n",
      "|    episodes         | 60       |\n",
      "|    fps              | 1799     |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 5323     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.462    |\n",
      "| time/               |          |\n",
      "|    episodes         | 64       |\n",
      "|    fps              | 1862     |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 5658     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.424    |\n",
      "| time/               |          |\n",
      "|    episodes         | 68       |\n",
      "|    fps              | 1931     |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 6059     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.388    |\n",
      "| time/               |          |\n",
      "|    episodes         | 72       |\n",
      "|    fps              | 1983     |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 6442     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.357    |\n",
      "| time/               |          |\n",
      "|    episodes         | 76       |\n",
      "|    fps              | 2035     |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 6773     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.318    |\n",
      "| time/               |          |\n",
      "|    episodes         | 80       |\n",
      "|    fps              | 2093     |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 7174     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.288    |\n",
      "| time/               |          |\n",
      "|    episodes         | 84       |\n",
      "|    fps              | 2127     |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 7493     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.253    |\n",
      "| time/               |          |\n",
      "|    episodes         | 88       |\n",
      "|    fps              | 2163     |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 7858     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.216    |\n",
      "| time/               |          |\n",
      "|    episodes         | 92       |\n",
      "|    fps              | 2196     |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 8249     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.188    |\n",
      "| time/               |          |\n",
      "|    episodes         | 96       |\n",
      "|    fps              | 2227     |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 8548     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.154    |\n",
      "| time/               |          |\n",
      "|    episodes         | 100      |\n",
      "|    fps              | 2266     |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 8908     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.116    |\n",
      "| time/               |          |\n",
      "|    episodes         | 104      |\n",
      "|    fps              | 2307     |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 9304     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.0746   |\n",
      "| time/               |          |\n",
      "|    episodes         | 108      |\n",
      "|    fps              | 2342     |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 9741     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 112      |\n",
      "|    fps              | 2364     |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 10060    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 116      |\n",
      "|    fps              | 2389     |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 10377    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 120      |\n",
      "|    fps              | 2417     |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 10701    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 124      |\n",
      "|    fps              | 2434     |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 11036    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 128      |\n",
      "|    fps              | 2453     |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 11416    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 132      |\n",
      "|    fps              | 2470     |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 11810    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 136      |\n",
      "|    fps              | 2494     |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 12159    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 140      |\n",
      "|    fps              | 2520     |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 12565    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 144      |\n",
      "|    fps              | 2535     |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total_timesteps  | 12929    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 148      |\n",
      "|    fps              | 2555     |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total_timesteps  | 13263    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 152      |\n",
      "|    fps              | 2574     |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total_timesteps  | 13580    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 156      |\n",
      "|    fps              | 2588     |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total_timesteps  | 13855    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 160      |\n",
      "|    fps              | 2607     |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total_timesteps  | 14230    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 164      |\n",
      "|    fps              | 2617     |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total_timesteps  | 14639    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 168      |\n",
      "|    fps              | 2631     |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total_timesteps  | 14914    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 172      |\n",
      "|    fps              | 2651     |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total_timesteps  | 15280    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 176      |\n",
      "|    fps              | 2666     |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total_timesteps  | 15693    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 180      |\n",
      "|    fps              | 2680     |\n",
      "|    time_elapsed     | 6        |\n",
      "|    total_timesteps  | 16128    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 184      |\n",
      "|    fps              | 2668     |\n",
      "|    time_elapsed     | 6        |\n",
      "|    total_timesteps  | 16492    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 188      |\n",
      "|    fps              | 2685     |\n",
      "|    time_elapsed     | 6        |\n",
      "|    total_timesteps  | 16839    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 192      |\n",
      "|    fps              | 2692     |\n",
      "|    time_elapsed     | 6        |\n",
      "|    total_timesteps  | 17191    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 196      |\n",
      "|    fps              | 2704     |\n",
      "|    time_elapsed     | 6        |\n",
      "|    total_timesteps  | 17527    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 200      |\n",
      "|    fps              | 2716     |\n",
      "|    time_elapsed     | 6        |\n",
      "|    total_timesteps  | 17857    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 204      |\n",
      "|    fps              | 2730     |\n",
      "|    time_elapsed     | 6        |\n",
      "|    total_timesteps  | 18272    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 208      |\n",
      "|    fps              | 2742     |\n",
      "|    time_elapsed     | 6        |\n",
      "|    total_timesteps  | 18636    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 212      |\n",
      "|    fps              | 2749     |\n",
      "|    time_elapsed     | 6        |\n",
      "|    total_timesteps  | 18972    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 216      |\n",
      "|    fps              | 2765     |\n",
      "|    time_elapsed     | 7        |\n",
      "|    total_timesteps  | 19362    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 220      |\n",
      "|    fps              | 2778     |\n",
      "|    time_elapsed     | 7        |\n",
      "|    total_timesteps  | 19766    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 224      |\n",
      "|    fps              | 2790     |\n",
      "|    time_elapsed     | 7        |\n",
      "|    total_timesteps  | 20079    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 228      |\n",
      "|    fps              | 2802     |\n",
      "|    time_elapsed     | 7        |\n",
      "|    total_timesteps  | 20491    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 232      |\n",
      "|    fps              | 2806     |\n",
      "|    time_elapsed     | 7        |\n",
      "|    total_timesteps  | 20854    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 236      |\n",
      "|    fps              | 2817     |\n",
      "|    time_elapsed     | 7        |\n",
      "|    total_timesteps  | 21204    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 240      |\n",
      "|    fps              | 2827     |\n",
      "|    time_elapsed     | 7        |\n",
      "|    total_timesteps  | 21557    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 244      |\n",
      "|    fps              | 2837     |\n",
      "|    time_elapsed     | 7        |\n",
      "|    total_timesteps  | 21946    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 248      |\n",
      "|    fps              | 2845     |\n",
      "|    time_elapsed     | 7        |\n",
      "|    total_timesteps  | 22304    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 252      |\n",
      "|    fps              | 2848     |\n",
      "|    time_elapsed     | 7        |\n",
      "|    total_timesteps  | 22593    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 256      |\n",
      "|    fps              | 2860     |\n",
      "|    time_elapsed     | 8        |\n",
      "|    total_timesteps  | 22937    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 260      |\n",
      "|    fps              | 2872     |\n",
      "|    time_elapsed     | 8        |\n",
      "|    total_timesteps  | 23306    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 264      |\n",
      "|    fps              | 2877     |\n",
      "|    time_elapsed     | 8        |\n",
      "|    total_timesteps  | 23623    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 268      |\n",
      "|    fps              | 2890     |\n",
      "|    time_elapsed     | 8        |\n",
      "|    total_timesteps  | 23968    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 272      |\n",
      "|    fps              | 2901     |\n",
      "|    time_elapsed     | 8        |\n",
      "|    total_timesteps  | 24284    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 276      |\n",
      "|    fps              | 2909     |\n",
      "|    time_elapsed     | 8        |\n",
      "|    total_timesteps  | 24665    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 280      |\n",
      "|    fps              | 2919     |\n",
      "|    time_elapsed     | 8        |\n",
      "|    total_timesteps  | 25039    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 284      |\n",
      "|    fps              | 2928     |\n",
      "|    time_elapsed     | 8        |\n",
      "|    total_timesteps  | 25389    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 288      |\n",
      "|    fps              | 2936     |\n",
      "|    time_elapsed     | 8        |\n",
      "|    total_timesteps  | 25680    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 292      |\n",
      "|    fps              | 2945     |\n",
      "|    time_elapsed     | 8        |\n",
      "|    total_timesteps  | 25990    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 296      |\n",
      "|    fps              | 2957     |\n",
      "|    time_elapsed     | 8        |\n",
      "|    total_timesteps  | 26360    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 300      |\n",
      "|    fps              | 2963     |\n",
      "|    time_elapsed     | 9        |\n",
      "|    total_timesteps  | 26734    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 304      |\n",
      "|    fps              | 2972     |\n",
      "|    time_elapsed     | 9        |\n",
      "|    total_timesteps  | 27041    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 308      |\n",
      "|    fps              | 2982     |\n",
      "|    time_elapsed     | 9        |\n",
      "|    total_timesteps  | 27391    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 312      |\n",
      "|    fps              | 2986     |\n",
      "|    time_elapsed     | 9        |\n",
      "|    total_timesteps  | 27722    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 316      |\n",
      "|    fps              | 2997     |\n",
      "|    time_elapsed     | 9        |\n",
      "|    total_timesteps  | 28121    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 320      |\n",
      "|    fps              | 3007     |\n",
      "|    time_elapsed     | 9        |\n",
      "|    total_timesteps  | 28498    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 324      |\n",
      "|    fps              | 3015     |\n",
      "|    time_elapsed     | 9        |\n",
      "|    total_timesteps  | 28838    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 328      |\n",
      "|    fps              | 3024     |\n",
      "|    time_elapsed     | 9        |\n",
      "|    total_timesteps  | 29250    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 332      |\n",
      "|    fps              | 3030     |\n",
      "|    time_elapsed     | 9        |\n",
      "|    total_timesteps  | 29660    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 336      |\n",
      "|    fps              | 3037     |\n",
      "|    time_elapsed     | 9        |\n",
      "|    total_timesteps  | 29989    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 340      |\n",
      "|    fps              | 3045     |\n",
      "|    time_elapsed     | 9        |\n",
      "|    total_timesteps  | 30277    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 344      |\n",
      "|    fps              | 3051     |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 30667    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 348      |\n",
      "|    fps              | 3057     |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 31015    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 352      |\n",
      "|    fps              | 3064     |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 31356    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 356      |\n",
      "|    fps              | 3067     |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 31734    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 360      |\n",
      "|    fps              | 3075     |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 32061    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 364      |\n",
      "|    fps              | 3083     |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 32405    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 368      |\n",
      "|    fps              | 3087     |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 32793    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 372      |\n",
      "|    fps              | 3093     |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 33147    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 376      |\n",
      "|    fps              | 3095     |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 33495    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 380      |\n",
      "|    fps              | 3103     |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 33872    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 384      |\n",
      "|    fps              | 3110     |\n",
      "|    time_elapsed     | 11       |\n",
      "|    total_timesteps  | 34262    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 388      |\n",
      "|    fps              | 3114     |\n",
      "|    time_elapsed     | 11       |\n",
      "|    total_timesteps  | 34646    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 392      |\n",
      "|    fps              | 3120     |\n",
      "|    time_elapsed     | 11       |\n",
      "|    total_timesteps  | 35036    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 396      |\n",
      "|    fps              | 3123     |\n",
      "|    time_elapsed     | 11       |\n",
      "|    total_timesteps  | 35341    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 400      |\n",
      "|    fps              | 3126     |\n",
      "|    time_elapsed     | 11       |\n",
      "|    total_timesteps  | 35723    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 404      |\n",
      "|    fps              | 3129     |\n",
      "|    time_elapsed     | 11       |\n",
      "|    total_timesteps  | 36028    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 408      |\n",
      "|    fps              | 3128     |\n",
      "|    time_elapsed     | 11       |\n",
      "|    total_timesteps  | 36452    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 412      |\n",
      "|    fps              | 3133     |\n",
      "|    time_elapsed     | 11       |\n",
      "|    total_timesteps  | 36811    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 416      |\n",
      "|    fps              | 3132     |\n",
      "|    time_elapsed     | 11       |\n",
      "|    total_timesteps  | 37200    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 420      |\n",
      "|    fps              | 3135     |\n",
      "|    time_elapsed     | 11       |\n",
      "|    total_timesteps  | 37562    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 424      |\n",
      "|    fps              | 3138     |\n",
      "|    time_elapsed     | 12       |\n",
      "|    total_timesteps  | 37906    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 428      |\n",
      "|    fps              | 3139     |\n",
      "|    time_elapsed     | 12       |\n",
      "|    total_timesteps  | 38281    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 432      |\n",
      "|    fps              | 3143     |\n",
      "|    time_elapsed     | 12       |\n",
      "|    total_timesteps  | 38746    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 436      |\n",
      "|    fps              | 3144     |\n",
      "|    time_elapsed     | 12       |\n",
      "|    total_timesteps  | 39146    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 440      |\n",
      "|    fps              | 3148     |\n",
      "|    time_elapsed     | 12       |\n",
      "|    total_timesteps  | 39533    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 444      |\n",
      "|    fps              | 3149     |\n",
      "|    time_elapsed     | 12       |\n",
      "|    total_timesteps  | 39935    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 448      |\n",
      "|    fps              | 3154     |\n",
      "|    time_elapsed     | 12       |\n",
      "|    total_timesteps  | 40318    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 452      |\n",
      "|    fps              | 3157     |\n",
      "|    time_elapsed     | 12       |\n",
      "|    total_timesteps  | 40710    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 456      |\n",
      "|    fps              | 3163     |\n",
      "|    time_elapsed     | 12       |\n",
      "|    total_timesteps  | 41082    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 460      |\n",
      "|    fps              | 3168     |\n",
      "|    time_elapsed     | 13       |\n",
      "|    total_timesteps  | 41402    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 464      |\n",
      "|    fps              | 3170     |\n",
      "|    time_elapsed     | 13       |\n",
      "|    total_timesteps  | 41740    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 468      |\n",
      "|    fps              | 3173     |\n",
      "|    time_elapsed     | 13       |\n",
      "|    total_timesteps  | 42073    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 472      |\n",
      "|    fps              | 3176     |\n",
      "|    time_elapsed     | 13       |\n",
      "|    total_timesteps  | 42444    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 476      |\n",
      "|    fps              | 3176     |\n",
      "|    time_elapsed     | 13       |\n",
      "|    total_timesteps  | 42774    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 480      |\n",
      "|    fps              | 3178     |\n",
      "|    time_elapsed     | 13       |\n",
      "|    total_timesteps  | 43193    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 484      |\n",
      "|    fps              | 3176     |\n",
      "|    time_elapsed     | 13       |\n",
      "|    total_timesteps  | 43527    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 488      |\n",
      "|    fps              | 3177     |\n",
      "|    time_elapsed     | 13       |\n",
      "|    total_timesteps  | 43897    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 492      |\n",
      "|    fps              | 3173     |\n",
      "|    time_elapsed     | 13       |\n",
      "|    total_timesteps  | 44211    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 496      |\n",
      "|    fps              | 3175     |\n",
      "|    time_elapsed     | 14       |\n",
      "|    total_timesteps  | 44552    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 500      |\n",
      "|    fps              | 3179     |\n",
      "|    time_elapsed     | 14       |\n",
      "|    total_timesteps  | 44869    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 504      |\n",
      "|    fps              | 3181     |\n",
      "|    time_elapsed     | 14       |\n",
      "|    total_timesteps  | 45269    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 508      |\n",
      "|    fps              | 3185     |\n",
      "|    time_elapsed     | 14       |\n",
      "|    total_timesteps  | 45582    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 512      |\n",
      "|    fps              | 3185     |\n",
      "|    time_elapsed     | 14       |\n",
      "|    total_timesteps  | 45896    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 516      |\n",
      "|    fps              | 3181     |\n",
      "|    time_elapsed     | 14       |\n",
      "|    total_timesteps  | 46238    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 520      |\n",
      "|    fps              | 3186     |\n",
      "|    time_elapsed     | 14       |\n",
      "|    total_timesteps  | 46573    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 524      |\n",
      "|    fps              | 3187     |\n",
      "|    time_elapsed     | 14       |\n",
      "|    total_timesteps  | 46963    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 528      |\n",
      "|    fps              | 3192     |\n",
      "|    time_elapsed     | 14       |\n",
      "|    total_timesteps  | 47330    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 532      |\n",
      "|    fps              | 3196     |\n",
      "|    time_elapsed     | 14       |\n",
      "|    total_timesteps  | 47686    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 536      |\n",
      "|    fps              | 3199     |\n",
      "|    time_elapsed     | 15       |\n",
      "|    total_timesteps  | 48116    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 540      |\n",
      "|    fps              | 3203     |\n",
      "|    time_elapsed     | 15       |\n",
      "|    total_timesteps  | 48484    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 544      |\n",
      "|    fps              | 3206     |\n",
      "|    time_elapsed     | 15       |\n",
      "|    total_timesteps  | 48787    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 548      |\n",
      "|    fps              | 3208     |\n",
      "|    time_elapsed     | 15       |\n",
      "|    total_timesteps  | 49230    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 552      |\n",
      "|    fps              | 3212     |\n",
      "|    time_elapsed     | 15       |\n",
      "|    total_timesteps  | 49597    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 556      |\n",
      "|    fps              | 3215     |\n",
      "|    time_elapsed     | 15       |\n",
      "|    total_timesteps  | 49969    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 560      |\n",
      "|    fps              | 3107     |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 50281    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 4.6      |\n",
      "|    n_updates        | 70       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 564      |\n",
      "|    fps              | 3019     |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 50575    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.33     |\n",
      "|    n_updates        | 143      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 568      |\n",
      "|    fps              | 2874     |\n",
      "|    time_elapsed     | 17       |\n",
      "|    total_timesteps  | 51163    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 4.78     |\n",
      "|    n_updates        | 290      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 572      |\n",
      "|    fps              | 2746     |\n",
      "|    time_elapsed     | 18       |\n",
      "|    total_timesteps  | 51769    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.945    |\n",
      "|    n_updates        | 442      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 576      |\n",
      "|    fps              | 2647     |\n",
      "|    time_elapsed     | 19       |\n",
      "|    total_timesteps  | 52312    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.71     |\n",
      "|    n_updates        | 577      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 580      |\n",
      "|    fps              | 2546     |\n",
      "|    time_elapsed     | 20       |\n",
      "|    total_timesteps  | 52890    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.08     |\n",
      "|    n_updates        | 722      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 584      |\n",
      "|    fps              | 2347     |\n",
      "|    time_elapsed     | 23       |\n",
      "|    total_timesteps  | 54105    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.678    |\n",
      "|    n_updates        | 1026     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 588      |\n",
      "|    fps              | 1712     |\n",
      "|    time_elapsed     | 33       |\n",
      "|    total_timesteps  | 58105    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.54     |\n",
      "|    n_updates        | 2026     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 592      |\n",
      "|    fps              | 1347     |\n",
      "|    time_elapsed     | 46       |\n",
      "|    total_timesteps  | 62105    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.08     |\n",
      "|    n_updates        | 3026     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 596      |\n",
      "|    fps              | 1116     |\n",
      "|    time_elapsed     | 59       |\n",
      "|    total_timesteps  | 66105    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.363    |\n",
      "|    n_updates        | 4026     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 600      |\n",
      "|    fps              | 964      |\n",
      "|    time_elapsed     | 72       |\n",
      "|    total_timesteps  | 70105    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 4.73     |\n",
      "|    n_updates        | 5026     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 604      |\n",
      "|    fps              | 865      |\n",
      "|    time_elapsed     | 85       |\n",
      "|    total_timesteps  | 74105    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.662    |\n",
      "|    n_updates        | 6026     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 608      |\n",
      "|    fps              | 795      |\n",
      "|    time_elapsed     | 98       |\n",
      "|    total_timesteps  | 78105    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.585    |\n",
      "|    n_updates        | 7026     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 612      |\n",
      "|    fps              | 749      |\n",
      "|    time_elapsed     | 109      |\n",
      "|    total_timesteps  | 82105    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.666    |\n",
      "|    n_updates        | 8026     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 616      |\n",
      "|    fps              | 691      |\n",
      "|    time_elapsed     | 124      |\n",
      "|    total_timesteps  | 86105    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.28     |\n",
      "|    n_updates        | 9026     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 620      |\n",
      "|    fps              | 660      |\n",
      "|    time_elapsed     | 136      |\n",
      "|    total_timesteps  | 90105    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 6.71     |\n",
      "|    n_updates        | 10026    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 624      |\n",
      "|    fps              | 640      |\n",
      "|    time_elapsed     | 146      |\n",
      "|    total_timesteps  | 94105    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.07     |\n",
      "|    n_updates        | 11026    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 628      |\n",
      "|    fps              | 614      |\n",
      "|    time_elapsed     | 159      |\n",
      "|    total_timesteps  | 98105    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.45     |\n",
      "|    n_updates        | 12026    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x7f381aaa3280>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=100_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762451d0",
   "metadata": {},
   "source": [
    "### 5. Testing Trained Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "edc05a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 Score: [-56.39176]\n",
      "Episode: 1 Score: [-125.84363]\n",
      "Episode: 2 Score: [-114.66706]\n",
      "Episode: 3 Score: [-109.27242]\n",
      "Episode: 4 Score: [-109.41255]\n"
     ]
    }
   ],
   "source": [
    "episodes = 5\n",
    "\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()\n",
    "    reward_cnt = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        action, _ = model.predict(state)\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        state = new_state\n",
    "        \n",
    "        reward_cnt += reward\n",
    "        \n",
    "    print(f\"Episode: {episode} Score: {reward_cnt}\")\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e01b3c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136e3072",
   "metadata": {},
   "source": [
    "### 6. Tune Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0e88cd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "52ffd73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_callback = StopTrainingOnRewardThreshold(reward_threshold=150, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1f4534ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join(\"train\", \"saved_models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6f2f3a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_callback = EvalCallback(env,\n",
    "                            callback_on_new_best=stop_callback,\n",
    "                            eval_freq=10_000,\n",
    "                            best_model_save_path=save_path,\n",
    "                            verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "66bd7508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "model = DQN(policy=\"MlpPolicy\", env=env, tensorboard_log=log_path, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "edbac65f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to train/logs/DQN_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prince/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=1000, episode_reward=-560.25 +/- 185.95\n",
      "Episode length: 127.00 +/- 14.45\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 127      |\n",
      "|    mean_reward      | -560     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.953    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1000     |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-471.06 +/- 109.45\n",
      "Episode length: 116.20 +/- 39.72\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 116      |\n",
      "|    mean_reward      | -471     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.905    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 2000     |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=-653.43 +/- 218.81\n",
      "Episode length: 188.80 +/- 83.90\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 189      |\n",
      "|    mean_reward      | -653     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.858    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 3000     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-578.60 +/- 246.27\n",
      "Episode length: 124.60 +/- 29.35\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 125      |\n",
      "|    mean_reward      | -579     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.81     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 4000     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-594.43 +/- 212.69\n",
      "Episode length: 142.40 +/- 51.34\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 142      |\n",
      "|    mean_reward      | -594     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.763    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 5000     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=-445.84 +/- 321.08\n",
      "Episode length: 117.20 +/- 56.23\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 117      |\n",
      "|    mean_reward      | -446     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.715    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 6000     |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=7000, episode_reward=-390.24 +/- 297.92\n",
      "Episode length: 105.60 +/- 17.53\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 106      |\n",
      "|    mean_reward      | -390     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.668    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 7000     |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8000, episode_reward=-579.47 +/- 226.37\n",
      "Episode length: 193.80 +/- 79.40\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 194      |\n",
      "|    mean_reward      | -579     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.62     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 8000     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=-584.53 +/- 186.13\n",
      "Episode length: 107.00 +/- 28.71\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 107      |\n",
      "|    mean_reward      | -585     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.573    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 9000     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-616.28 +/- 152.92\n",
      "Episode length: 146.20 +/- 47.71\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 146      |\n",
      "|    mean_reward      | -616     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.525    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 10000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=-609.61 +/- 184.66\n",
      "Episode length: 116.20 +/- 21.83\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 116      |\n",
      "|    mean_reward      | -610     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.478    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 11000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=-395.33 +/- 80.42\n",
      "Episode length: 119.80 +/- 33.62\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 120      |\n",
      "|    mean_reward      | -395     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.43     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 12000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=-585.23 +/- 163.60\n",
      "Episode length: 194.60 +/- 67.94\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 195      |\n",
      "|    mean_reward      | -585     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.383    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 13000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=-664.01 +/- 204.63\n",
      "Episode length: 151.80 +/- 29.84\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 152      |\n",
      "|    mean_reward      | -664     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.335    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 14000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=-632.80 +/- 204.54\n",
      "Episode length: 132.80 +/- 45.26\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 133      |\n",
      "|    mean_reward      | -633     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.288    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 15000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=-612.71 +/- 248.49\n",
      "Episode length: 196.80 +/- 106.96\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 197      |\n",
      "|    mean_reward      | -613     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.24     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 16000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=-609.56 +/- 252.58\n",
      "Episode length: 158.80 +/- 66.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 159      |\n",
      "|    mean_reward      | -610     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.193    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 17000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=-513.57 +/- 153.37\n",
      "Episode length: 184.00 +/- 72.20\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 184      |\n",
      "|    mean_reward      | -514     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.145    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 18000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=-469.14 +/- 207.64\n",
      "Episode length: 111.80 +/- 37.88\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 112      |\n",
      "|    mean_reward      | -469     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.0975   |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 19000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=-451.48 +/- 141.31\n",
      "Episode length: 129.80 +/- 22.37\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 130      |\n",
      "|    mean_reward      | -451     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 20000    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=21000, episode_reward=-567.50 +/- 263.93\n",
      "Episode length: 126.40 +/- 45.51\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 126      |\n",
      "|    mean_reward      | -567     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 21000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=-327.48 +/- 141.28\n",
      "Episode length: 112.60 +/- 52.58\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 113      |\n",
      "|    mean_reward      | -327     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 22000    |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=23000, episode_reward=-410.01 +/- 125.48\n",
      "Episode length: 105.20 +/- 26.66\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 105      |\n",
      "|    mean_reward      | -410     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 23000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=-484.23 +/- 192.86\n",
      "Episode length: 158.80 +/- 104.05\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 159      |\n",
      "|    mean_reward      | -484     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 24000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=-569.09 +/- 211.56\n",
      "Episode length: 115.80 +/- 36.13\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 116      |\n",
      "|    mean_reward      | -569     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 25000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=26000, episode_reward=-626.71 +/- 262.53\n",
      "Episode length: 200.60 +/- 85.52\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 201      |\n",
      "|    mean_reward      | -627     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 26000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=27000, episode_reward=-513.77 +/- 177.62\n",
      "Episode length: 147.20 +/- 52.82\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 147      |\n",
      "|    mean_reward      | -514     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 27000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=28000, episode_reward=-486.07 +/- 157.58\n",
      "Episode length: 136.00 +/- 13.86\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 136      |\n",
      "|    mean_reward      | -486     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 28000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=29000, episode_reward=-679.30 +/- 192.34\n",
      "Episode length: 144.40 +/- 56.88\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 144      |\n",
      "|    mean_reward      | -679     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 29000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=-634.60 +/- 204.67\n",
      "Episode length: 110.80 +/- 20.94\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 111      |\n",
      "|    mean_reward      | -635     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 30000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=31000, episode_reward=-454.41 +/- 237.32\n",
      "Episode length: 145.20 +/- 67.68\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 145      |\n",
      "|    mean_reward      | -454     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 31000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=32000, episode_reward=-616.18 +/- 141.64\n",
      "Episode length: 122.00 +/- 34.62\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 122      |\n",
      "|    mean_reward      | -616     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 32000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=33000, episode_reward=-567.98 +/- 250.74\n",
      "Episode length: 139.80 +/- 27.08\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 140      |\n",
      "|    mean_reward      | -568     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 33000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=34000, episode_reward=-660.98 +/- 218.60\n",
      "Episode length: 119.00 +/- 23.84\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 119      |\n",
      "|    mean_reward      | -661     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 34000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=35000, episode_reward=-481.53 +/- 303.56\n",
      "Episode length: 146.20 +/- 90.79\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 146      |\n",
      "|    mean_reward      | -482     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 35000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=36000, episode_reward=-393.49 +/- 160.26\n",
      "Episode length: 124.60 +/- 44.91\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 125      |\n",
      "|    mean_reward      | -393     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 36000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=37000, episode_reward=-588.40 +/- 147.77\n",
      "Episode length: 148.80 +/- 62.18\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 149      |\n",
      "|    mean_reward      | -588     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 37000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=38000, episode_reward=-583.60 +/- 136.55\n",
      "Episode length: 136.40 +/- 33.36\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 136      |\n",
      "|    mean_reward      | -584     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 38000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=39000, episode_reward=-505.72 +/- 210.55\n",
      "Episode length: 149.40 +/- 77.18\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 149      |\n",
      "|    mean_reward      | -506     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 39000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=-588.96 +/- 147.09\n",
      "Episode length: 128.20 +/- 47.18\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 128      |\n",
      "|    mean_reward      | -589     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 40000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=41000, episode_reward=-495.64 +/- 341.80\n",
      "Episode length: 124.80 +/- 40.67\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 125      |\n",
      "|    mean_reward      | -496     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 41000    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=42000, episode_reward=-576.75 +/- 142.63\n",
      "Episode length: 143.60 +/- 64.79\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 144      |\n",
      "|    mean_reward      | -577     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 42000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=43000, episode_reward=-674.64 +/- 148.29\n",
      "Episode length: 120.80 +/- 17.63\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 121      |\n",
      "|    mean_reward      | -675     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 43000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=44000, episode_reward=-703.29 +/- 194.83\n",
      "Episode length: 143.40 +/- 49.34\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 143      |\n",
      "|    mean_reward      | -703     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 44000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=45000, episode_reward=-474.18 +/- 279.46\n",
      "Episode length: 168.80 +/- 66.83\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 169      |\n",
      "|    mean_reward      | -474     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 45000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=46000, episode_reward=-642.05 +/- 129.61\n",
      "Episode length: 138.40 +/- 25.14\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 138      |\n",
      "|    mean_reward      | -642     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 46000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=47000, episode_reward=-468.53 +/- 335.70\n",
      "Episode length: 106.00 +/- 33.72\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 106      |\n",
      "|    mean_reward      | -469     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 47000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=48000, episode_reward=-337.81 +/- 140.44\n",
      "Episode length: 104.40 +/- 37.54\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 104      |\n",
      "|    mean_reward      | -338     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 48000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=49000, episode_reward=-571.25 +/- 185.42\n",
      "Episode length: 103.80 +/- 25.10\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 104      |\n",
      "|    mean_reward      | -571     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 49000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=-542.60 +/- 158.65\n",
      "Episode length: 116.40 +/- 50.40\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 116      |\n",
      "|    mean_reward      | -543     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 50000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=51000, episode_reward=-662.95 +/- 214.84\n",
      "Episode length: 99.00 +/- 28.64\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 99       |\n",
      "|    mean_reward      | -663     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 51000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.34     |\n",
      "|    n_updates        | 249      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=52000, episode_reward=-771.89 +/- 233.41\n",
      "Episode length: 111.00 +/- 32.59\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 111      |\n",
      "|    mean_reward      | -772     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 52000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.3      |\n",
      "|    n_updates        | 499      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=53000, episode_reward=-374.42 +/- 132.15\n",
      "Episode length: 265.20 +/- 92.70\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 265      |\n",
      "|    mean_reward      | -374     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 53000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.574    |\n",
      "|    n_updates        | 749      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=54000, episode_reward=-315.69 +/- 88.75\n",
      "Episode length: 258.60 +/- 71.87\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 259      |\n",
      "|    mean_reward      | -316     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 54000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.982    |\n",
      "|    n_updates        | 999      |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=55000, episode_reward=-230.64 +/- 72.86\n",
      "Episode length: 231.40 +/- 68.87\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 231      |\n",
      "|    mean_reward      | -231     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 55000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 4.01     |\n",
      "|    n_updates        | 1249     |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=56000, episode_reward=-312.65 +/- 54.87\n",
      "Episode length: 432.20 +/- 139.49\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 432      |\n",
      "|    mean_reward      | -313     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 56000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.56     |\n",
      "|    n_updates        | 1499     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=57000, episode_reward=-209.63 +/- 220.15\n",
      "Episode length: 533.00 +/- 123.44\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 533      |\n",
      "|    mean_reward      | -210     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 57000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 4.49     |\n",
      "|    n_updates        | 1749     |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=58000, episode_reward=-250.34 +/- 108.30\n",
      "Episode length: 401.80 +/- 203.28\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 402      |\n",
      "|    mean_reward      | -250     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 58000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.929    |\n",
      "|    n_updates        | 1999     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=59000, episode_reward=-120.77 +/- 120.12\n",
      "Episode length: 285.20 +/- 52.81\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 285      |\n",
      "|    mean_reward      | -121     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 59000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.34     |\n",
      "|    n_updates        | 2249     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-148.73 +/- 53.24\n",
      "Episode length: 424.60 +/- 189.99\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 425      |\n",
      "|    mean_reward      | -149     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 60000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.554    |\n",
      "|    n_updates        | 2499     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=61000, episode_reward=-207.59 +/- 73.48\n",
      "Episode length: 749.80 +/- 306.76\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 750      |\n",
      "|    mean_reward      | -208     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 61000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.95     |\n",
      "|    n_updates        | 2749     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=62000, episode_reward=-147.33 +/- 67.67\n",
      "Episode length: 500.20 +/- 250.55\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 500      |\n",
      "|    mean_reward      | -147     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 62000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.78     |\n",
      "|    n_updates        | 2999     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=63000, episode_reward=-145.69 +/- 103.47\n",
      "Episode length: 736.80 +/- 332.99\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 737      |\n",
      "|    mean_reward      | -146     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 63000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.689    |\n",
      "|    n_updates        | 3249     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=64000, episode_reward=-182.02 +/- 106.57\n",
      "Episode length: 500.20 +/- 265.51\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 500      |\n",
      "|    mean_reward      | -182     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 64000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.917    |\n",
      "|    n_updates        | 3499     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=65000, episode_reward=-183.71 +/- 73.57\n",
      "Episode length: 440.00 +/- 314.92\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 440      |\n",
      "|    mean_reward      | -184     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 65000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.846    |\n",
      "|    n_updates        | 3749     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=66000, episode_reward=-188.82 +/- 55.73\n",
      "Episode length: 538.40 +/- 382.95\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 538      |\n",
      "|    mean_reward      | -189     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 66000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.629    |\n",
      "|    n_updates        | 3999     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=67000, episode_reward=-73.84 +/- 12.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -73.8    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 67000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.728    |\n",
      "|    n_updates        | 4249     |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=68000, episode_reward=-119.15 +/- 66.95\n",
      "Episode length: 337.00 +/- 332.95\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 337      |\n",
      "|    mean_reward      | -119     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 68000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.02     |\n",
      "|    n_updates        | 4499     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=69000, episode_reward=-211.41 +/- 75.15\n",
      "Episode length: 273.40 +/- 151.07\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 273      |\n",
      "|    mean_reward      | -211     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 69000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.62     |\n",
      "|    n_updates        | 4749     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=70000, episode_reward=-116.27 +/- 48.96\n",
      "Episode length: 656.20 +/- 421.25\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 656      |\n",
      "|    mean_reward      | -116     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 70000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.625    |\n",
      "|    n_updates        | 4999     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=71000, episode_reward=-121.56 +/- 28.62\n",
      "Episode length: 823.60 +/- 352.80\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 824      |\n",
      "|    mean_reward      | -122     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 71000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.795    |\n",
      "|    n_updates        | 5249     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=72000, episode_reward=-173.26 +/- 152.77\n",
      "Episode length: 356.60 +/- 329.84\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 357      |\n",
      "|    mean_reward      | -173     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 72000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.785    |\n",
      "|    n_updates        | 5499     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=73000, episode_reward=-138.93 +/- 81.67\n",
      "Episode length: 650.40 +/- 428.25\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 650      |\n",
      "|    mean_reward      | -139     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 73000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.847    |\n",
      "|    n_updates        | 5749     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=74000, episode_reward=-134.46 +/- 89.72\n",
      "Episode length: 486.20 +/- 420.65\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 486      |\n",
      "|    mean_reward      | -134     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 74000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.867    |\n",
      "|    n_updates        | 5999     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=75000, episode_reward=-112.06 +/- 22.20\n",
      "Episode length: 827.80 +/- 344.40\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 828      |\n",
      "|    mean_reward      | -112     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 75000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.542    |\n",
      "|    n_updates        | 6249     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=76000, episode_reward=-96.25 +/- 13.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -96.3    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 76000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.495    |\n",
      "|    n_updates        | 6499     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=77000, episode_reward=-183.62 +/- 83.22\n",
      "Episode length: 475.20 +/- 429.05\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 475      |\n",
      "|    mean_reward      | -184     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 77000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.588    |\n",
      "|    n_updates        | 6749     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=78000, episode_reward=-192.11 +/- 116.33\n",
      "Episode length: 307.80 +/- 347.88\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 308      |\n",
      "|    mean_reward      | -192     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 78000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.802    |\n",
      "|    n_updates        | 6999     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=79000, episode_reward=-194.47 +/- 97.89\n",
      "Episode length: 489.60 +/- 416.86\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 490      |\n",
      "|    mean_reward      | -194     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 79000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.316    |\n",
      "|    n_updates        | 7249     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=-289.93 +/- 129.39\n",
      "Episode length: 513.20 +/- 397.76\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 513      |\n",
      "|    mean_reward      | -290     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 80000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.928    |\n",
      "|    n_updates        | 7499     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=81000, episode_reward=-243.58 +/- 106.29\n",
      "Episode length: 327.20 +/- 336.91\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 327      |\n",
      "|    mean_reward      | -244     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 81000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.796    |\n",
      "|    n_updates        | 7749     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=82000, episode_reward=-237.55 +/- 73.64\n",
      "Episode length: 337.60 +/- 334.74\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 338      |\n",
      "|    mean_reward      | -238     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 82000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.576    |\n",
      "|    n_updates        | 7999     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=83000, episode_reward=-181.68 +/- 86.15\n",
      "Episode length: 659.80 +/- 418.24\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 660      |\n",
      "|    mean_reward      | -182     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 83000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.583    |\n",
      "|    n_updates        | 8249     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=84000, episode_reward=-112.03 +/- 71.04\n",
      "Episode length: 333.40 +/- 336.88\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 333      |\n",
      "|    mean_reward      | -112     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 84000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.77     |\n",
      "|    n_updates        | 8499     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=85000, episode_reward=-279.54 +/- 202.63\n",
      "Episode length: 655.60 +/- 421.81\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 656      |\n",
      "|    mean_reward      | -280     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 85000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.927    |\n",
      "|    n_updates        | 8749     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=86000, episode_reward=-192.65 +/- 105.15\n",
      "Episode length: 656.00 +/- 421.34\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 656      |\n",
      "|    mean_reward      | -193     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 86000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.765    |\n",
      "|    n_updates        | 8999     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=87000, episode_reward=-218.93 +/- 119.90\n",
      "Episode length: 762.20 +/- 329.16\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 762      |\n",
      "|    mean_reward      | -219     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 87000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1        |\n",
      "|    n_updates        | 9249     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=88000, episode_reward=-158.27 +/- 91.06\n",
      "Episode length: 546.40 +/- 371.26\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 546      |\n",
      "|    mean_reward      | -158     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 88000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.596    |\n",
      "|    n_updates        | 9499     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=89000, episode_reward=-169.59 +/- 91.72\n",
      "Episode length: 682.60 +/- 388.78\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 683      |\n",
      "|    mean_reward      | -170     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 89000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.03     |\n",
      "|    n_updates        | 9749     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=90000, episode_reward=-282.91 +/- 127.53\n",
      "Episode length: 544.60 +/- 376.58\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 545      |\n",
      "|    mean_reward      | -283     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 90000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.557    |\n",
      "|    n_updates        | 9999     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=91000, episode_reward=-126.97 +/- 108.00\n",
      "Episode length: 502.40 +/- 407.65\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 502      |\n",
      "|    mean_reward      | -127     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 91000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.617    |\n",
      "|    n_updates        | 10249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=92000, episode_reward=-133.47 +/- 27.73\n",
      "Episode length: 843.00 +/- 314.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 843      |\n",
      "|    mean_reward      | -133     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 92000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.71     |\n",
      "|    n_updates        | 10499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=93000, episode_reward=-204.49 +/- 125.01\n",
      "Episode length: 339.20 +/- 333.47\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 339      |\n",
      "|    mean_reward      | -204     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 93000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.808    |\n",
      "|    n_updates        | 10749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=94000, episode_reward=-142.02 +/- 80.49\n",
      "Episode length: 700.20 +/- 367.21\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 700      |\n",
      "|    mean_reward      | -142     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 94000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.04     |\n",
      "|    n_updates        | 10999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=95000, episode_reward=-163.72 +/- 49.10\n",
      "Episode length: 679.60 +/- 392.70\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 680      |\n",
      "|    mean_reward      | -164     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 95000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 4.18     |\n",
      "|    n_updates        | 11249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=96000, episode_reward=-206.98 +/- 86.27\n",
      "Episode length: 706.80 +/- 359.47\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 707      |\n",
      "|    mean_reward      | -207     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 96000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.898    |\n",
      "|    n_updates        | 11499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=97000, episode_reward=-155.68 +/- 72.38\n",
      "Episode length: 548.60 +/- 370.44\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 549      |\n",
      "|    mean_reward      | -156     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 97000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 4        |\n",
      "|    n_updates        | 11749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=98000, episode_reward=-166.70 +/- 89.04\n",
      "Episode length: 546.00 +/- 371.56\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 546      |\n",
      "|    mean_reward      | -167     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 98000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.592    |\n",
      "|    n_updates        | 11999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=99000, episode_reward=-157.51 +/- 66.25\n",
      "Episode length: 870.80 +/- 258.40\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 871      |\n",
      "|    mean_reward      | -158     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 99000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.719    |\n",
      "|    n_updates        | 12249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=-160.11 +/- 49.90\n",
      "Episode length: 761.00 +/- 307.41\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 761      |\n",
      "|    mean_reward      | -160     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 100000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.544    |\n",
      "|    n_updates        | 12499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=101000, episode_reward=-106.18 +/- 4.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -106     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 101000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.83     |\n",
      "|    n_updates        | 12749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=102000, episode_reward=-89.75 +/- 41.57\n",
      "Episode length: 852.00 +/- 296.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 852      |\n",
      "|    mean_reward      | -89.7    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 102000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.75     |\n",
      "|    n_updates        | 12999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=103000, episode_reward=-18.56 +/- 90.24\n",
      "Episode length: 803.40 +/- 248.09\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 803      |\n",
      "|    mean_reward      | -18.6    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 103000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.69     |\n",
      "|    n_updates        | 13249    |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=104000, episode_reward=-169.66 +/- 143.15\n",
      "Episode length: 861.20 +/- 277.60\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 861      |\n",
      "|    mean_reward      | -170     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 104000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.587    |\n",
      "|    n_updates        | 13499    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=105000, episode_reward=-195.52 +/- 161.40\n",
      "Episode length: 864.00 +/- 272.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 864      |\n",
      "|    mean_reward      | -196     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 105000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.599    |\n",
      "|    n_updates        | 13749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=106000, episode_reward=-116.65 +/- 23.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -117     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 106000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.77     |\n",
      "|    n_updates        | 13999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=107000, episode_reward=-197.38 +/- 83.99\n",
      "Episode length: 851.00 +/- 193.03\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 851      |\n",
      "|    mean_reward      | -197     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 107000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.791    |\n",
      "|    n_updates        | 14249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=108000, episode_reward=-134.73 +/- 70.30\n",
      "Episode length: 937.20 +/- 125.60\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 937      |\n",
      "|    mean_reward      | -135     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 108000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.694    |\n",
      "|    n_updates        | 14499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=109000, episode_reward=-134.55 +/- 26.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -135     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 109000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.588    |\n",
      "|    n_updates        | 14749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=110000, episode_reward=-96.51 +/- 72.99\n",
      "Episode length: 711.40 +/- 353.46\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 711      |\n",
      "|    mean_reward      | -96.5    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 110000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.581    |\n",
      "|    n_updates        | 14999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=111000, episode_reward=-123.82 +/- 19.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -124     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 111000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.503    |\n",
      "|    n_updates        | 15249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=112000, episode_reward=-107.41 +/- 12.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -107     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 112000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.18     |\n",
      "|    n_updates        | 15499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=113000, episode_reward=-90.65 +/- 14.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -90.6    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 113000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.754    |\n",
      "|    n_updates        | 15749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=114000, episode_reward=-121.46 +/- 23.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -121     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 114000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.72     |\n",
      "|    n_updates        | 15999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=115000, episode_reward=-108.02 +/- 19.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -108     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 115000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.09     |\n",
      "|    n_updates        | 16249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=116000, episode_reward=-103.81 +/- 20.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -104     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 116000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.473    |\n",
      "|    n_updates        | 16499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=117000, episode_reward=-104.91 +/- 19.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -105     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 117000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.25     |\n",
      "|    n_updates        | 16749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=118000, episode_reward=-109.35 +/- 12.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -109     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 118000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.42     |\n",
      "|    n_updates        | 16999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=119000, episode_reward=-121.54 +/- 37.57\n",
      "Episode length: 903.00 +/- 194.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 903      |\n",
      "|    mean_reward      | -122     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 119000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.636    |\n",
      "|    n_updates        | 17249    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=120000, episode_reward=-123.92 +/- 14.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -124     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 120000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.501    |\n",
      "|    n_updates        | 17499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=121000, episode_reward=-112.67 +/- 21.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -113     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 121000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.886    |\n",
      "|    n_updates        | 17749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=122000, episode_reward=-85.52 +/- 13.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -85.5    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 122000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.08     |\n",
      "|    n_updates        | 17999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=123000, episode_reward=-105.20 +/- 16.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -105     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 123000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.38     |\n",
      "|    n_updates        | 18249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=124000, episode_reward=-109.82 +/- 15.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -110     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 124000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.908    |\n",
      "|    n_updates        | 18499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=125000, episode_reward=-98.85 +/- 17.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -98.8    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 125000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.843    |\n",
      "|    n_updates        | 18749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=126000, episode_reward=-102.00 +/- 15.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -102     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 126000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.669    |\n",
      "|    n_updates        | 18999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=127000, episode_reward=-106.62 +/- 31.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -107     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 127000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.499    |\n",
      "|    n_updates        | 19249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=128000, episode_reward=-157.25 +/- 25.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -157     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 128000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 6.68     |\n",
      "|    n_updates        | 19499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=129000, episode_reward=-93.81 +/- 23.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -93.8    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 129000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.726    |\n",
      "|    n_updates        | 19749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=130000, episode_reward=-93.82 +/- 10.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -93.8    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 130000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.06     |\n",
      "|    n_updates        | 19999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=131000, episode_reward=-113.20 +/- 28.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -113     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 131000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.725    |\n",
      "|    n_updates        | 20249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=132000, episode_reward=-123.71 +/- 30.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -124     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 132000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.409    |\n",
      "|    n_updates        | 20499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=133000, episode_reward=-103.18 +/- 15.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -103     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 133000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.682    |\n",
      "|    n_updates        | 20749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=134000, episode_reward=-85.82 +/- 14.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -85.8    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 134000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.696    |\n",
      "|    n_updates        | 20999    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=135000, episode_reward=-103.61 +/- 12.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -104     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 135000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.617    |\n",
      "|    n_updates        | 21249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=136000, episode_reward=-120.45 +/- 12.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -120     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 136000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.712    |\n",
      "|    n_updates        | 21499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=137000, episode_reward=-97.75 +/- 23.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -97.8    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 137000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.794    |\n",
      "|    n_updates        | 21749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=138000, episode_reward=-100.95 +/- 15.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -101     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 138000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.517    |\n",
      "|    n_updates        | 21999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=139000, episode_reward=-77.81 +/- 28.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -77.8    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 139000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.641    |\n",
      "|    n_updates        | 22249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=140000, episode_reward=-126.27 +/- 12.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -126     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 140000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.511    |\n",
      "|    n_updates        | 22499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=141000, episode_reward=-108.50 +/- 31.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -109     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 141000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.656    |\n",
      "|    n_updates        | 22749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=142000, episode_reward=-128.23 +/- 53.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -128     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 142000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.05     |\n",
      "|    n_updates        | 22999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=143000, episode_reward=-111.27 +/- 17.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -111     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 143000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.565    |\n",
      "|    n_updates        | 23249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=144000, episode_reward=-109.40 +/- 23.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -109     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 144000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.503    |\n",
      "|    n_updates        | 23499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=145000, episode_reward=-112.91 +/- 27.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -113     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 145000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.787    |\n",
      "|    n_updates        | 23749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=146000, episode_reward=-114.97 +/- 9.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -115     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 146000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.741    |\n",
      "|    n_updates        | 23999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=147000, episode_reward=-107.24 +/- 27.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -107     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 147000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.691    |\n",
      "|    n_updates        | 24249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=148000, episode_reward=-108.63 +/- 33.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -109     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 148000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.887    |\n",
      "|    n_updates        | 24499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=149000, episode_reward=-118.79 +/- 13.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -119     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 149000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.852    |\n",
      "|    n_updates        | 24749    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=150000, episode_reward=-93.07 +/- 19.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -93.1    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 150000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.57     |\n",
      "|    n_updates        | 24999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=151000, episode_reward=-92.31 +/- 18.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -92.3    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 151000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.853    |\n",
      "|    n_updates        | 25249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=152000, episode_reward=-110.70 +/- 23.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -111     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 152000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.509    |\n",
      "|    n_updates        | 25499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=153000, episode_reward=-80.83 +/- 21.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -80.8    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 153000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.589    |\n",
      "|    n_updates        | 25749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=154000, episode_reward=-93.35 +/- 31.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -93.4    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 154000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.52     |\n",
      "|    n_updates        | 25999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=155000, episode_reward=-98.37 +/- 10.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -98.4    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 155000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.767    |\n",
      "|    n_updates        | 26249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=156000, episode_reward=-131.09 +/- 47.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -131     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 156000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.72     |\n",
      "|    n_updates        | 26499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=157000, episode_reward=-89.32 +/- 11.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -89.3    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 157000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.53     |\n",
      "|    n_updates        | 26749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=158000, episode_reward=-102.77 +/- 25.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -103     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 158000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.33     |\n",
      "|    n_updates        | 26999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=159000, episode_reward=-101.84 +/- 16.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -102     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 159000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.438    |\n",
      "|    n_updates        | 27249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=160000, episode_reward=-109.78 +/- 13.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -110     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 160000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.59     |\n",
      "|    n_updates        | 27499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=161000, episode_reward=-81.30 +/- 17.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -81.3    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 161000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.596    |\n",
      "|    n_updates        | 27749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=162000, episode_reward=-112.11 +/- 13.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -112     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 162000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.525    |\n",
      "|    n_updates        | 27999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=163000, episode_reward=-83.59 +/- 29.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -83.6    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 163000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.706    |\n",
      "|    n_updates        | 28249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=164000, episode_reward=-128.98 +/- 13.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -129     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 164000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.406    |\n",
      "|    n_updates        | 28499    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=165000, episode_reward=-116.51 +/- 30.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -117     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 165000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.708    |\n",
      "|    n_updates        | 28749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=166000, episode_reward=-82.70 +/- 26.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -82.7    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 166000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.488    |\n",
      "|    n_updates        | 28999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=167000, episode_reward=-153.58 +/- 66.94\n",
      "Episode length: 999.80 +/- 0.40\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -154     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 167000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.528    |\n",
      "|    n_updates        | 29249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=168000, episode_reward=-131.59 +/- 32.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -132     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 168000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.406    |\n",
      "|    n_updates        | 29499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=169000, episode_reward=-110.82 +/- 30.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -111     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 169000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.768    |\n",
      "|    n_updates        | 29749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=170000, episode_reward=-100.79 +/- 19.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -101     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 170000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.583    |\n",
      "|    n_updates        | 29999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=171000, episode_reward=-101.40 +/- 30.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -101     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 171000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.646    |\n",
      "|    n_updates        | 30249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=172000, episode_reward=-88.28 +/- 30.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -88.3    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 172000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.645    |\n",
      "|    n_updates        | 30499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=173000, episode_reward=-96.85 +/- 15.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -96.9    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 173000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.505    |\n",
      "|    n_updates        | 30749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=174000, episode_reward=-110.29 +/- 26.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -110     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 174000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.707    |\n",
      "|    n_updates        | 30999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=175000, episode_reward=-101.73 +/- 29.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -102     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 175000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.848    |\n",
      "|    n_updates        | 31249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=176000, episode_reward=-103.20 +/- 16.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -103     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 176000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.605    |\n",
      "|    n_updates        | 31499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=177000, episode_reward=-111.91 +/- 31.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -112     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 177000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.547    |\n",
      "|    n_updates        | 31749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=178000, episode_reward=-79.13 +/- 20.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -79.1    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 178000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.781    |\n",
      "|    n_updates        | 31999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=179000, episode_reward=-95.60 +/- 39.92\n",
      "Episode length: 978.40 +/- 43.20\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 978      |\n",
      "|    mean_reward      | -95.6    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 179000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.52     |\n",
      "|    n_updates        | 32249    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=180000, episode_reward=-99.44 +/- 47.27\n",
      "Episode length: 994.00 +/- 12.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 994      |\n",
      "|    mean_reward      | -99.4    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 180000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 10.4     |\n",
      "|    n_updates        | 32499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=181000, episode_reward=-118.49 +/- 17.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -118     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 181000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.589    |\n",
      "|    n_updates        | 32749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=182000, episode_reward=-141.31 +/- 66.64\n",
      "Episode length: 987.40 +/- 25.20\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 987      |\n",
      "|    mean_reward      | -141     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 182000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.317    |\n",
      "|    n_updates        | 32999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=183000, episode_reward=-110.69 +/- 42.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -111     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 183000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.597    |\n",
      "|    n_updates        | 33249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=184000, episode_reward=-155.63 +/- 25.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -156     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 184000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.678    |\n",
      "|    n_updates        | 33499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=185000, episode_reward=-135.67 +/- 44.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -136     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 185000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.395    |\n",
      "|    n_updates        | 33749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=186000, episode_reward=-131.72 +/- 40.54\n",
      "Episode length: 996.20 +/- 7.60\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 996      |\n",
      "|    mean_reward      | -132     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 186000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.79     |\n",
      "|    n_updates        | 33999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=187000, episode_reward=-109.25 +/- 21.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -109     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 187000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 4.24     |\n",
      "|    n_updates        | 34249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=188000, episode_reward=-97.18 +/- 25.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -97.2    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 188000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.506    |\n",
      "|    n_updates        | 34499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=189000, episode_reward=-92.22 +/- 35.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -92.2    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 189000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.476    |\n",
      "|    n_updates        | 34749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=190000, episode_reward=-97.41 +/- 30.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -97.4    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 190000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.58     |\n",
      "|    n_updates        | 34999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=191000, episode_reward=-121.84 +/- 45.68\n",
      "Episode length: 981.20 +/- 37.60\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 981      |\n",
      "|    mean_reward      | -122     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 191000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.717    |\n",
      "|    n_updates        | 35249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=192000, episode_reward=-91.73 +/- 25.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -91.7    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 192000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.404    |\n",
      "|    n_updates        | 35499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=193000, episode_reward=-130.07 +/- 33.26\n",
      "Episode length: 973.40 +/- 53.20\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 973      |\n",
      "|    mean_reward      | -130     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 193000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.682    |\n",
      "|    n_updates        | 35749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=194000, episode_reward=-84.47 +/- 28.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -84.5    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 194000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.592    |\n",
      "|    n_updates        | 35999    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=195000, episode_reward=-102.64 +/- 21.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -103     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 195000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.449    |\n",
      "|    n_updates        | 36249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=196000, episode_reward=-93.57 +/- 27.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -93.6    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 196000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.58     |\n",
      "|    n_updates        | 36499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=197000, episode_reward=-102.76 +/- 49.34\n",
      "Episode length: 989.20 +/- 21.60\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 989      |\n",
      "|    mean_reward      | -103     |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 197000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.466    |\n",
      "|    n_updates        | 36749    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=198000, episode_reward=-95.10 +/- 30.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -95.1    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 198000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.432    |\n",
      "|    n_updates        | 36999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=199000, episode_reward=-77.32 +/- 31.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -77.3    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 199000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.609    |\n",
      "|    n_updates        | 37249    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=200000, episode_reward=-99.56 +/- 25.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1e+03    |\n",
      "|    mean_reward      | -99.6    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 200000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.15     |\n",
      "|    n_updates        | 37499    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x7f37ceb056d0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=200_000, log_interval=10_000, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95fb731",
   "metadata": {},
   "source": [
    "### 7. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f68bfe3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = os.path.join(\"train\", \"saved_model\", \"DQN_lunar_lander_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ee8fcf34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prince/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/stable_baselines3/common/save_util.py:276: UserWarning: Path 'train/saved_model' does not exist. Will create it.\n",
      "  warnings.warn(f\"Path '{path.parent}' does not exist. Will create it.\")\n"
     ]
    }
   ],
   "source": [
    "model.save(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d225a3",
   "metadata": {},
   "source": [
    "### 8. Delete Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "08841a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9dd475",
   "metadata": {},
   "source": [
    "### 9. Reload Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c48eb9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DQN.load(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39d2108",
   "metadata": {},
   "source": [
    "### 10. Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "55055b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prince/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-108.02370287410595, 31.90873397019299)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_policy(model, env, n_eval_episodes=10, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "be5a45d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9202094",
   "metadata": {},
   "source": [
    "### 11. View Performance On Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "067f4654",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: | \n",
      "The environment is inconsistent, please check the package plan carefully\n",
      "The following packages are causing the inconsistency:\n",
      "/ \n",
      "  - defaults/linux-64::tensorflow-gpu==2.4.1=h30adc30_0\n",
      "  - defaults/linux-64::tensorflow==2.4.1=gpu_py39h8236f22_0\n",
      "  - defaults/linux-64::tensorflow-base==2.4.1=gpu_py39h29c2da4_0\n",
      "  - defaults/linux-64::hdf5==1.10.6=hb1b8bf9_0\n",
      "  - defaults/linux-64::scipy==1.7.3=py39hc147768_0\n",
      "  - defaults/linux-64::libgfortran-ng==7.5.0=ha8ba4b0_17\n",
      "  - anaconda/noarch::seaborn==0.11.2=pyhd3eb1b0_0\n",
      "  - defaults/noarch::keras-preprocessing==1.1.2=pyhd3eb1b0_0\n",
      "  - defaults/linux-64::h5py==2.10.0=py39hec9cf62_0\n",
      "done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.8.2\n",
      "  latest version: 4.13.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/prince/anaconda3/envs/tf-gpu\n",
      "\n",
      "  added / updated specs:\n",
      "    - tensorboard\n",
      "\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  ca-certificates    pkgs/main::ca-certificates-2022.4.26-~ --> conda-forge::ca-certificates-2022.6.15-ha878542_0\n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  certifi            pkgs/main::certifi-2022.6.15-py39h06a~ --> conda-forge::certifi-2022.6.15-py39hf3d152e_0\n",
      "\n",
      "\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n"
     ]
    }
   ],
   "source": [
    "!conda install -c conda-forge tensorboard -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ef1c6123",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'train/logs/DQN_2'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_log_path = os.path.join(log_path, \"DQN_2\")\n",
    "training_log_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "df7b925f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "368734a8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-09 13:49:41.692690: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.4.0 at http://localhost:6006/ (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir={training_log_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5940a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
