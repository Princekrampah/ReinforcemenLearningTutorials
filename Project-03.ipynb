{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "850b6877",
   "metadata": {},
   "source": [
    "# Cart Pole"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f5da35",
   "metadata": {},
   "source": [
    "### 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5f50f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym[classic_control] in /home/prince/anaconda3/envs/tf-gpu/lib/python3.9/site-packages (0.21.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/prince/anaconda3/envs/tf-gpu/lib/python3.9/site-packages (from gym[classic_control]) (2.1.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /home/prince/anaconda3/envs/tf-gpu/lib/python3.9/site-packages (from gym[classic_control]) (1.22.3)\n",
      "Requirement already satisfied: pyglet>=1.4.0 in /home/prince/anaconda3/envs/tf-gpu/lib/python3.9/site-packages (from gym[classic_control]) (1.5.26)\n"
     ]
    }
   ],
   "source": [
    "!pip install gym[classic_control]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbeac17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3 import PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54880e0e",
   "metadata": {},
   "source": [
    "### 2. Load Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da68169a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79797bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 Score: 38.0\n",
      "Episode: 1 Score: 18.0\n",
      "Episode: 2 Score: 17.0\n",
      "Episode: 3 Score: 13.0\n",
      "Episode: 4 Score: 14.0\n",
      "Episode: 5 Score: 10.0\n",
      "Episode: 6 Score: 39.0\n",
      "Episode: 7 Score: 16.0\n",
      "Episode: 8 Score: 28.0\n",
      "Episode: 9 Score: 28.0\n"
     ]
    }
   ],
   "source": [
    "episodes = 10\n",
    "\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    reward_cnt = 0\n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        reward_cnt += reward\n",
    "        state = new_state\n",
    "        \n",
    "    print(f\"Episode: {episode} Score: {reward_cnt}\")\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1379477",
   "metadata": {},
   "source": [
    "### 3. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f450fc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = os.path.join(\"train\", \"logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b341a88a",
   "metadata": {},
   "source": [
    "**PPO**: Proximal Policy Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa4b37f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "env = DummyVecEnv([lambda: env])\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21170432",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-09 14:07:07.901271: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "<frozen importlib._bootstrap>:228: RuntimeWarning: numpy.ndarray size changed, may indicate binary incompatibility. Expected 96, got 80\n",
      "<frozen importlib._bootstrap>:228: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 232, got 216\n",
      "<frozen importlib._bootstrap>:228: RuntimeWarning: numpy.ndarray size changed, may indicate binary incompatibility. Expected 80 from C header, got 96 from PyObject\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to train/logs/PPO_1\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 487  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 4    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 474         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008661149 |\n",
      "|    clip_fraction        | 0.0976      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.686      |\n",
      "|    explained_variance   | -0.00507    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.63        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0144     |\n",
      "|    value_loss           | 51.7        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 430        |\n",
      "|    iterations           | 3          |\n",
      "|    time_elapsed         | 14         |\n",
      "|    total_timesteps      | 6144       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00853833 |\n",
      "|    clip_fraction        | 0.0472     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.668     |\n",
      "|    explained_variance   | 0.0943     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 13.7       |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | -0.0151    |\n",
      "|    value_loss           | 41.1       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 413         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 19          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008364185 |\n",
      "|    clip_fraction        | 0.0897      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.636      |\n",
      "|    explained_variance   | 0.242       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 16.6        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0215     |\n",
      "|    value_loss           | 50.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 401         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 25          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008830335 |\n",
      "|    clip_fraction        | 0.0904      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.603      |\n",
      "|    explained_variance   | 0.291       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 31.5        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0203     |\n",
      "|    value_loss           | 60.5        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7f8b543bddf0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "345f5532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 Score: [158.]\n",
      "Episode: 1 Score: [177.]\n",
      "Episode: 2 Score: [208.]\n",
      "Episode: 3 Score: [181.]\n",
      "Episode: 4 Score: [313.]\n",
      "Episode: 5 Score: [387.]\n",
      "Episode: 6 Score: [235.]\n",
      "Episode: 7 Score: [231.]\n",
      "Episode: 8 Score: [333.]\n",
      "Episode: 9 Score: [189.]\n"
     ]
    }
   ],
   "source": [
    "episodes = 10\n",
    "\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    reward_cnt = 0\n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        action, _ = model.predict(state)\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        reward_cnt += reward\n",
    "        state = new_state\n",
    "        \n",
    "    print(f\"Episode: {episode} Score: {reward_cnt}\")\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467b4939",
   "metadata": {},
   "source": [
    "### 4. Change Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6cd7042",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_arch = [dict(pi=[128, 128, 128, 128], vf=[128, 128, 128, 128])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8cd9bc6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "model = PPO(\"MlpPolicy\", env, verbose=1, tensorboard_log=log_path, policy_kwargs={\"net_arch\": net_arch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "94acbdd6",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to train/logs/PPO_2\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 611  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 3    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 394         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015773349 |\n",
      "|    clip_fraction        | 0.23        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.681      |\n",
      "|    explained_variance   | -0.00107    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.13        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0237     |\n",
      "|    value_loss           | 18.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 369         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013827929 |\n",
      "|    clip_fraction        | 0.154       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.652      |\n",
      "|    explained_variance   | 0.419       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 12.2        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0276     |\n",
      "|    value_loss           | 29.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 382         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 21          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009947624 |\n",
      "|    clip_fraction        | 0.134       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.609      |\n",
      "|    explained_variance   | 0.459       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 19.4        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0273     |\n",
      "|    value_loss           | 45.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 389         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015263688 |\n",
      "|    clip_fraction        | 0.152       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.565      |\n",
      "|    explained_variance   | 0.538       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.79        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0214     |\n",
      "|    value_loss           | 34.9        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7f8a6bfb27c0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=10_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03afa9e",
   "metadata": {},
   "source": [
    "### 5. Add Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d35f66c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "835d2090",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join(\"train\", \"saved_models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a6498be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_callback = StopTrainingOnRewardThreshold(reward_threshold=475, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d8244c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_callback = EvalCallback(env,\n",
    "                            callback_on_new_best=stop_callback,\n",
    "                            best_model_save_path=save_path,\n",
    "                            verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "10646c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO(\"MlpPolicy\", env=env, tensorboard_log=log_path, policy_kwargs={\"net_arch\": net_arch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "812bdf55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prince/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=10000, episode_reward=377.60 +/- 99.67\n",
      "Episode length: 377.60 +/- 99.67\n",
      "New best mean reward!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7f8a6bf7d280>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=10_000, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9200c970",
   "metadata": {},
   "source": [
    "### 6. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2a9ac472",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = os.path.join(\"train\", \"saved_models\", \"PPO_cart_pole_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d79f4d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adadb1fc",
   "metadata": {},
   "source": [
    "### 7. Delete Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c8880088",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265e6a2c",
   "metadata": {},
   "source": [
    "### 8. Reload Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d22e80a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(MODEL_PATH, env=env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a105f641",
   "metadata": {},
   "source": [
    "### 9. Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4628eded",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prince/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(450.3, 77.43907282502806)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_policy(model, env=env, n_eval_episodes=10, render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9ef1fe",
   "metadata": {},
   "source": [
    "### 10. View Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2c0dbf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8d802621",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_log_path = os.path.join(log_path, \"PPO_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7a5e6421",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-09 14:25:32.458129: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.4.0 at http://localhost:6006/ (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir={training_log_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd41b10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
